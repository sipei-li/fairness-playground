{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF, TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import combinations\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import svds\n",
    "import implicit\n",
    "\n",
    "import random\n",
    "my_seed = 0\n",
    "random.seed(my_seed)\n",
    "np.random.seed(my_seed)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# ground_truth: list of items ordered by time\n",
    "def nDCG_Time(ground_truth, _recList):\n",
    "    rec_num = len(_recList) # topK\n",
    "    # ground_truth is already sorted by time\n",
    "    idealOrder = ground_truth\n",
    "    idealDCG = 0.0\n",
    "    for j in range(min(rec_num, len(idealOrder))):\n",
    "        idealDCG += ((math.pow(2.0, len(idealOrder) - j) - 1) / math.log(2.0 + j))\n",
    "\n",
    "    recDCG = 0.0\n",
    "    for j in range(rec_num):\n",
    "        item = _recList[j]\n",
    "        if item in ground_truth:\n",
    "            rank = len(ground_truth) - ground_truth.index(item) # why ground truth?\n",
    "            recDCG += ((math.pow(2.0, rank) - 1) / math.log(1.0 + j + 1))\n",
    "\n",
    "    return (recDCG / idealDCG)\n",
    "\n",
    "\n",
    "def Recall(_test_set, _recList):\n",
    "    hit = len(set(_recList).intersection(set(_test_set)))\n",
    "    # return hit / float(len(_test_set))\n",
    "    return hit / min(float(len(_test_set)), float(len(_recList)))\n",
    "\n",
    "def Precision(_test_set, _recList):\n",
    "    hit = len(set(_recList).intersection(set(_test_set)))\n",
    "    return hit / float(len(_recList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for local\n",
    "#listening_df = pd.read_csv('data/lastfm/listening_events.tsv', header=1, sep='\\t',\n",
    "#                      names=['user_id', 'track_id', 'album_id', 'timestamp'])\n",
    "#user_df = pd.read_csv('data/lastfm/users.tsv', header=1, sep='\\t',\n",
    "#                      names=['user_id', 'country', 'age', 'gender', 'creation_time'])\n",
    "listening_df = pd.read_csv('./data/lastfm_2020/listening_events_2020.tsv', header=1, sep='\\t',\n",
    "                           names=['user_id', 'track_id', 'album_id', 'timestamp'])\n",
    "user_df = pd.read_csv('./data/lastfm_2020/users_2020.tsv', header=1, sep='\\t',\n",
    "                     names=['user_id', 'country', 'age', 'gender', 'creation_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for server\n",
    "listening_df = pd.read_csv('/data/sli21/lastfm/listening_events.tsv', header=1, sep='\\t',\n",
    "                      names=['user_id', 'track_id', 'album_id', 'timestamp'])\n",
    "user_df = pd.read_csv('/data/sli21/lastfm/users.tsv', header=1, sep='\\t',\n",
    "                      names=['user_id', 'country', 'age', 'gender', 'creation_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "listening_users = listening_df['user_id'].unique()\n",
    "filed_users = user_df['user_id'].unique()\n",
    "\n",
    "for id in listening_users:\n",
    "    if id not in filed_users:\n",
    "        print(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user with id 2 is not in the `user_df`, so we delete their record from `listening_df` as well.\n",
    "listening_df = listening_df[listening_df['user_id'] != 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['n', 'm', 'f', nan], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df['gender'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of female users: 2372\n",
      "Number of male users: 9950\n"
     ]
    }
   ],
   "source": [
    "f_users = user_df[user_df['gender'] == 'f']\n",
    "m_users = user_df[user_df['gender'] == 'm']\n",
    "print('Number of female users: {}\\nNumber of male users: {}'.format(f_users.shape[0], m_users.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_mat(df, user_n, item_n, user_id_to_iid, item_id_to_iid):\n",
    "    \"\"\"\n",
    "    Convert DataFrame to sparse matrix.\n",
    "\n",
    "    Arg:\n",
    "        df: DataFrame, ratings dataframe with user_id, movie_id and rating\n",
    "\n",
    "    Return:\n",
    "        mat: scipy.sparse.csr_matrix, sparse ratings matrix with rows being users and cols being items\n",
    "    \"\"\"\n",
    "    \n",
    "    mat = sparse.lil_matrix((user_n, item_n))\n",
    "    for _, row in df.iterrows():\n",
    "        user_id = int(row[0])\n",
    "        item_id = int(row[1])\n",
    "        user_iid = user_id_to_iid[user_id]\n",
    "        item_iid = item_id_to_iid[item_id]\n",
    "        mat[user_iid, item_iid] = 1\n",
    "    \n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cf_recommend(user_id, topk, user_id_to_iid, item_iid_to_id, train_mat, est_mat):\n",
    "    \n",
    "    user_iid = user_id_to_iid[user_id]\n",
    "    user_interactions = train_mat[user_iid, :]\n",
    "    interacted_before = np.nonzero(user_interactions)[1]\n",
    "    estimations = est_mat[user_iid, :].copy()\n",
    "    estimations[interacted_before] = 0\n",
    "\n",
    "    top_item_iids = np.argsort(-estimations)[:topk]\n",
    "    top_item_ids = [item_iid_to_id[i] for i in top_item_iids]\n",
    "\n",
    "    return (user_id, np.array(top_item_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cf_experiment(n_epochs, listening_df, user_df):\n",
    "    \n",
    "    all_f_cf_r = []\n",
    "    all_m_cf_r = []\n",
    "\n",
    "    for _ in range(n_epochs):\n",
    "        # small_listening_df = listening_df.sample(frac=0.005, ignore_index=True) #1/200 of dataset\n",
    "        small_listening_df = listening_df.sample(frac=0.005)  #ignore_index removed for pandas version < 1.3.0\n",
    "        # small_listening_df = listening_df #1/1 of dataset\n",
    "\n",
    "        user_n = small_listening_df['user_id'].nunique()\n",
    "        item_n = small_listening_df['track_id'].nunique()\n",
    "\n",
    "        user_ids = small_listening_df['user_id'].unique()\n",
    "        item_ids = small_listening_df['track_id'].unique()\n",
    "\n",
    "        user_id_to_iid = {user_ids[i]:i for i in range(len(user_ids))}\n",
    "        user_iid_to_id = {i:user_ids[i] for i in range(len(user_ids))}\n",
    "\n",
    "        item_id_to_iid = {item_ids[i]:i for i in range(len(item_ids))}\n",
    "        item_iid_to_id = {i:item_ids[i] for i in range(len(item_ids))}\n",
    "\n",
    "        gender_df = pd.merge(user_df, small_listening_df, on='user_id')[['user_id', 'gender']]\n",
    "        f_user_ids = gender_df[gender_df['gender'] == 'f']['user_id'].unique()\n",
    "        m_user_ids = gender_df[gender_df['gender'] == 'm']['user_id'].unique()\n",
    "\n",
    "        train_df, test_df = train_test_split(small_listening_df, test_size=0.2)\n",
    "\n",
    "        train_mat = df_to_mat(train_df, user_n, item_n, user_id_to_iid, item_id_to_iid)\n",
    "        train_mat = train_mat.tocsr()\n",
    "\n",
    "        # mf = NMF(n_components=10, init='random', random_state=0, max_iter=500, verbose=False)\n",
    "        # mf = TruncatedSVD(n_components=10, algorithm='arpack', tol=0.0)\n",
    "        # user_f = mf.fit_transform(train_mat)\n",
    "        # item_f = mf.components_.T\n",
    "        # est_mat = np.dot(user_f, item_f.T)\n",
    "        \"\"\"\n",
    "        user_svd = TruncatedSVD(n_components=10, algorithm='arpack', tol=0.0)\n",
    "        user_f = user_svd.fit_transform(train_mat)\n",
    "        item_svd = TruncatedSVD(n_components=10, algorithm='arpack', tol=0.0)\n",
    "        item_f = item_svd.fit_transform(train_mat.transpose())\n",
    "        est_mat = np.dot(user_f, item_f.T)\n",
    "        \"\"\"\n",
    "        mf = implicit.als.AlternatingLeastSquares(factors=10, regularization=0.05, alpha=2.0)\n",
    "        mf.fit(train_mat)\n",
    "        user_f = mf.user_factors\n",
    "        item_f = mf.item_factors\n",
    "        est_mat = np.dot(user_f, item_f.T)\n",
    "\n",
    "        test_mat = df_to_mat(test_df, user_n, item_n, user_id_to_iid, item_id_to_iid)\n",
    "        test_mat = test_mat.tocsr()\n",
    "        \n",
    "        f_cf_r = []\n",
    "        for user_id in f_user_ids:\n",
    "            user_iid = user_id_to_iid[user_id]\n",
    "            test_item_iids = list(np.argwhere(test_mat[user_iid] > 0)[:, 1])\n",
    "            test_item_ids = list(map(lambda x: item_iid_to_id[x], test_item_iids))\n",
    "\n",
    "            if len(test_item_ids) > 0:\n",
    "                top_item_ids = list(cf_recommend(user_id, 10, user_id_to_iid, item_iid_to_id, train_mat, est_mat)[1])\n",
    "\n",
    "                recall = Recall(test_item_ids, top_item_ids)\n",
    "                f_cf_r.append(recall)\n",
    "        \n",
    "        all_f_cf_r.append(np.average(f_cf_r))\n",
    "\n",
    "        m_cf_r = []\n",
    "        for user_id in m_user_ids:\n",
    "            user_iid = user_id_to_iid[user_id]\n",
    "            test_item_iids = list(np.argwhere(test_mat[user_iid] > 0)[:, 1])\n",
    "            test_item_ids = list(map(lambda x: item_iid_to_id[x], test_item_iids))\n",
    "\n",
    "            if len(test_item_ids) > 0:\n",
    "                top_item_ids = list(cf_recommend(user_id, 10, user_id_to_iid, item_iid_to_id, train_mat, est_mat)[1])\n",
    "\n",
    "                recall = Recall(test_item_ids, top_item_ids)\n",
    "                m_cf_r.append(recall)\n",
    "        \n",
    "        all_m_cf_r.append(np.average(m_cf_r))\n",
    "        print(all_f_cf_r, all_m_cf_r)\n",
    "    \n",
    "    return (all_f_cf_r, all_m_cf_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/sli21/.local/lib/python3.9/site-packages/implicit/utils.py:28: UserWarning: OpenBLAS detected. Its highly recommend to set the environment variable 'export OPENBLAS_NUM_THREADS=1' to disable its internal multithreading\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "922a70dab8d94a6fbfe237d6b3f69ef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.001954661114966458] [0.0008094667480413025]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a9d2aec31dc4b8092e227d0d0a97e5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.001954661114966458, 0.0006958942240779402] [0.0008094667480413025, 0.0008147408147408146]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97487291204c424bb6fe16e8562c64c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.001954661114966458, 0.0006958942240779402, 0.0016294227188081935] [0.0008094667480413025, 0.0008147408147408146, 0.0019797198533894507]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ece91d50d440ca90bb494bb32f73e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.001954661114966458, 0.0006958942240779402, 0.0016294227188081935, 0.0017997685185185187] [0.0008094667480413025, 0.0008147408147408146, 0.0019797198533894507, 0.0010060249559629595]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a56a3953f840129ec7bba795721017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.001954661114966458, 0.0006958942240779402, 0.0016294227188081935, 0.0017997685185185187, 0.0006514842510763652] [0.0008094667480413025, 0.0008147408147408146, 0.0019797198533894507, 0.0010060249559629595, 0.0009034057443786162]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4259809d13743f39f4bab3534bf5e4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.001954661114966458, 0.0006958942240779402, 0.0016294227188081935, 0.0017997685185185187, 0.0006514842510763652, 0.0005771006463527238] [0.0008094667480413025, 0.0008147408147408146, 0.0019797198533894507, 0.0010060249559629595, 0.0009034057443786162, 0.00018838304552590268]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9a2ac58e53442788741280d17f4c578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.001954661114966458, 0.0006958942240779402, 0.0016294227188081935, 0.0017997685185185187, 0.0006514842510763652, 0.0005771006463527238, 0.0009449562957713206] [0.0008094667480413025, 0.0008147408147408146, 0.0019797198533894507, 0.0010060249559629595, 0.0009034057443786162, 0.00018838304552590268, 0.001001999555654299]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cd96098b17e421faadcfe820c536e21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.001954661114966458, 0.0006958942240779402, 0.0016294227188081935, 0.0017997685185185187, 0.0006514842510763652, 0.0005771006463527238, 0.0009449562957713206, 0.0014229064613949147] [0.0008094667480413025, 0.0008147408147408146, 0.0019797198533894507, 0.0010060249559629595, 0.0009034057443786162, 0.00018838304552590268, 0.001001999555654299, 0.0017776556776556774]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63adc8635fa047c78b4fa2d3297839d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.001954661114966458, 0.0006958942240779402, 0.0016294227188081935, 0.0017997685185185187, 0.0006514842510763652, 0.0005771006463527238, 0.0009449562957713206, 0.0014229064613949147, 0.0016315161161957817] [0.0008094667480413025, 0.0008147408147408146, 0.0019797198533894507, 0.0010060249559629595, 0.0009034057443786162, 0.00018838304552590268, 0.001001999555654299, 0.0017776556776556774, 0.0015249316811570242]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c80a54960d74eea9013cde587249f5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.001954661114966458, 0.0006958942240779402, 0.0016294227188081935, 0.0017997685185185187, 0.0006514842510763652, 0.0005771006463527238, 0.0009449562957713206, 0.0014229064613949147, 0.0016315161161957817, 0.0009330534173081408] [0.0008094667480413025, 0.0008147408147408146, 0.0019797198533894507, 0.0010060249559629595, 0.0009034057443786162, 0.00018838304552590268, 0.001001999555654299, 0.0017776556776556774, 0.0015249316811570242, 0.0005960378983634797]\n"
     ]
    }
   ],
   "source": [
    "cf_results = cf_experiment(10, listening_df, user_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_f_cf_r = np.array(cf_results[0])\n",
    "all_m_cf_r = np.array(cf_results[1])\n",
    "cf_fairness_scores = np.abs(all_f_cf_r - all_m_cf_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cf_experiment_implicit(n_epochs, listening_df, user_df):\n",
    "    \n",
    "    all_f_cf_r = []\n",
    "    all_m_cf_r = []\n",
    "\n",
    "    for _ in range(n_epochs):\n",
    "        # small_listening_df = listening_df.sample(frac=0.005, ignore_index=True) #1/200 of dataset\n",
    "        small_listening_df = listening_df.sample(frac=0.1)\n",
    "        # small_listening_df = listenting_df\n",
    "\n",
    "        user_n = small_listening_df['user_id'].nunique()\n",
    "        item_n = small_listening_df['track_id'].nunique()\n",
    "\n",
    "        user_ids = small_listening_df['user_id'].unique()\n",
    "        item_ids = small_listening_df['track_id'].unique()\n",
    "\n",
    "        user_id_to_iid = {user_ids[i]:i for i in range(len(user_ids))}\n",
    "        user_iid_to_id = {i:user_ids[i] for i in range(len(user_ids))}\n",
    "\n",
    "        item_id_to_iid = {item_ids[i]:i for i in range(len(item_ids))}\n",
    "        item_iid_to_id = {i:item_ids[i] for i in range(len(item_ids))}\n",
    "\n",
    "        gender_df = pd.merge(user_df, small_listening_df, on='user_id')[['user_id', 'gender']]\n",
    "        f_user_ids = gender_df[gender_df['gender'] == 'f']['user_id'].unique()\n",
    "        m_user_ids = gender_df[gender_df['gender'] == 'm']['user_id'].unique()\n",
    "\n",
    "        train_df, test_df = train_test_split(small_listening_df, test_size=0.2)\n",
    "\n",
    "        train_mat = df_to_mat(train_df, user_n, item_n, user_id_to_iid, item_id_to_iid)\n",
    "        train_mat = train_mat.tocsr()\n",
    "\n",
    "        mf = implicit.als.AlternatingLeastSquares(factors=100, regularization=0.00, alpha=5.0) #\n",
    "        mf.fit(train_mat)\n",
    "        user_f = mf.user_factors\n",
    "        item_f = mf.item_factors\n",
    "        est_mat = np.dot(user_f, item_f.T)\n",
    "\n",
    "        test_mat = df_to_mat(test_df, user_n, item_n, user_id_to_iid, item_id_to_iid)\n",
    "        test_mat = test_mat.tocsr()\n",
    "        \n",
    "        f_cf_r = []\n",
    "        for user_id in f_user_ids:\n",
    "            user_iid = user_id_to_iid[user_id]\n",
    "            test_item_iids = list(np.argwhere(test_mat[user_iid] > 0)[:, 1])\n",
    "            test_item_ids = list(map(lambda x: item_iid_to_id[x], test_item_iids))\n",
    "\n",
    "            if len(test_item_ids) > 0:\n",
    "                top_item_iids = list(mf.recommend(user_iid, train_mat[user_iid], N=10, filter_already_liked_items=True)[0])\n",
    "                top_item_ids = [item_iid_to_id[i] for i in top_item_iids]\n",
    "\n",
    "                recall = Recall(test_item_ids, top_item_ids)\n",
    "                f_cf_r.append(recall)\n",
    "        \n",
    "        all_f_cf_r.append(np.average(f_cf_r))\n",
    "\n",
    "        m_cf_r = []\n",
    "        for user_id in m_user_ids:\n",
    "            user_iid = user_id_to_iid[user_id]\n",
    "            test_item_iids = list(np.argwhere(test_mat[user_iid] > 0)[:, 1])\n",
    "            test_item_ids = list(map(lambda x: item_iid_to_id[x], test_item_iids))\n",
    "\n",
    "            if len(test_item_ids) > 0:\n",
    "                top_item_iids = list(mf.recommend(user_iid, train_mat[user_iid], N=10, filter_already_liked_items=True)[0])\n",
    "                top_item_ids = [item_iid_to_id[i] for i in top_item_iids]\n",
    "                \n",
    "                recall = Recall(test_item_ids, top_item_ids)\n",
    "                m_cf_r.append(recall)\n",
    "        \n",
    "        all_m_cf_r.append(np.average(m_cf_r))\n",
    "        # print(np.average(f_cf_r), np.average(m_cf_r))\n",
    "    \n",
    "    return (all_f_cf_r, all_m_cf_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_result_implicit = cf_experiment_implicit(1, listening_df, user_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'female recall: {np.average(cf_result_implicit[0])}, std: {np.std(cf_result_implicit[0])}; male recall: {np.average(cf_result_implicit[1])}, std: {np.std(cf_result_implicit[1])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### content-based filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_json_lst = []\n",
    "with open('data/lastfm/tags.json', 'r', encoding='utf-8') as f:\n",
    "    for obj in f:\n",
    "        track_dict = json.loads(obj)\n",
    "        track_json_lst.append(track_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_tags_lst = []\n",
    "for obj in track_json_lst:\n",
    "    track_id = obj['i']\n",
    "    tags = list(obj['tags'].keys())[:10]\n",
    "    track_tags_lst.append([track_id, tags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_df = pd.DataFrame(track_tags_lst, columns=['track_id', 'tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cb_recommend(user_id, topk, knn, item_user_mat, X, user_id_to_iid, item_iid_to_id, item_n):\n",
    "    user_iid = user_id_to_iid[user_id]\n",
    "    user_ratings = item_user_mat[:, user_iid]\n",
    "    rated_before = np.nonzero(user_ratings)[0]\n",
    "    sorted_rated_before = rated_before[\n",
    "        np.argsort(user_ratings[rated_before].toarray().squeeze())][::-1]\n",
    "    \n",
    "    if sorted_rated_before.size > 0:\n",
    "\n",
    "        raw_recommends = {}\n",
    "        for item_iid in sorted_rated_before:\n",
    "            distances, indices = knn.kneighbors(X[item_iid], \n",
    "                                                n_neighbors=topk+1)\n",
    "            sorted_pairs = sorted(list(zip(indices.squeeze().tolist(),\n",
    "                                           distances.squeeze().tolist())),\n",
    "                                  key=lambda x: x[1])\n",
    "            raw_recommends[item_iid] = sorted_pairs \n",
    "        \n",
    "        top_item_ids = []\n",
    "        pos = 0\n",
    "        while True:\n",
    "            for item_iid in sorted_rated_before:\n",
    "                next_neighbor_iid = raw_recommends[item_iid][pos][0]\n",
    "                if next_neighbor_iid not in rated_before:\n",
    "                    top_item_ids.append(item_iid_to_id[next_neighbor_iid])\n",
    "                if len(top_item_ids) > topk - 1:\n",
    "                    return (user_id, np.array(top_item_ids))\n",
    "            \n",
    "            pos += 1\n",
    "    else:\n",
    "\n",
    "        top_item_ids = list(map(lambda x: item_iid_to_id[x], \n",
    "                             random.sample(list(range(0, item_n)), topk)))\n",
    "        return (user_id, np.array(top_item_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_evaluate(test_user_ids, knn, X, user_id_to_iid, item_iid_to_id, train_mat, test_mat, item_n):\n",
    "\n",
    "    r = []\n",
    "\n",
    "    for user_id in test_user_ids:\n",
    "        user_iid = user_id_to_iid[user_id]\n",
    "        test_item_iids = list(np.argwhere(test_mat[:, user_iid] > 0)[:, 0])\n",
    "        test_item_ids = list(map(lambda x: item_iid_to_id[x], test_item_iids))\n",
    "\n",
    "        if len(test_item_ids) > 0:\n",
    "            top_item_ids = list(cb_recommend(user_id, 10, knn, train_mat, X, user_id_to_iid, item_iid_to_id, item_n)[1])\n",
    "\n",
    "            recall = Recall(test_item_ids, top_item_ids)\n",
    "\n",
    "            r.append(recall)\n",
    "    \n",
    "    return np.average(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cb_experiment(n_epochs, n_iters, listening_df, user_df, tag_df):\n",
    "\n",
    "    all_f_cb_r = []\n",
    "    all_m_cb_r = []\n",
    "    \n",
    "    tagged_listening_df = pd.merge(listening_df, tag_df, on='track_id')\n",
    "\n",
    "    for _ in range(n_epochs):\n",
    "        small_listening_df = tagged_listening_df.sample(frac=0.02, ignore_index=True)\n",
    "\n",
    "        user_n = small_listening_df['user_id'].nunique()\n",
    "        item_n = small_listening_df['track_id'].nunique()\n",
    "\n",
    "        user_ids = small_listening_df['user_id'].unique()\n",
    "        item_ids = small_listening_df['track_id'].unique()\n",
    "\n",
    "        user_id_to_iid = {user_ids[i]:i for i in range(len(user_ids))}\n",
    "        user_iid_to_id = {i:user_ids[i] for i in range(len(user_ids))}\n",
    "\n",
    "        item_id_to_iid = {item_ids[i]:i for i in range(len(item_ids))}\n",
    "        item_iid_to_id = {i:item_ids[i] for i in range(len(item_ids))}\n",
    "\n",
    "        gender_df = pd.merge(user_df, small_listening_df, on='user_id')[['user_id', 'gender']]\n",
    "        f_user_ids = gender_df[gender_df['gender'] == 'f']['user_id'].unique()\n",
    "        m_user_ids = gender_df[gender_df['gender'] == 'm']['user_id'].unique()\n",
    "\n",
    "        small_tag_df = small_listening_df.drop_duplicates(subset=['track_id'])[['track_id', 'tags']]\n",
    "        tf = TfidfVectorizer(analyzer = lambda x: (g for g in x))\n",
    "        X_tfidf = tf.fit_transform(small_tag_df['tags'])\n",
    "\n",
    "        knn = NearestNeighbors(metric='cosine', algorithm='auto', n_neighbors=10, n_jobs=-1)\n",
    "        knn.fit(X_tfidf)\n",
    "\n",
    "        train_df, test_df = train_test_split(small_listening_df, test_size=0.2)\n",
    "\n",
    "        train_mat = df_to_mat(train_df, user_n, item_n, user_id_to_iid, item_id_to_iid)\n",
    "        train_mat = train_mat.transpose().tocsr()\n",
    "\n",
    "        test_mat = df_to_mat(test_df, user_n, item_n, user_id_to_iid, item_id_to_iid)\n",
    "        test_mat= test_mat.transpose().tocsr()\n",
    "\n",
    "        \n",
    "        for _ in range(n_iters):\n",
    "\n",
    "            test_f_user_ids = np.random.choice(f_user_ids, size=500, replace=False)\n",
    "            test_m_user_ids = np.random.choice(m_user_ids, size=500, replace=False)\n",
    "\n",
    "            f_cb_r = sample_evaluate(test_f_user_ids, knn, X_tfidf, user_id_to_iid, item_iid_to_id, train_mat, test_mat, item_n)\n",
    "            m_cb_r = sample_evaluate(test_m_user_ids, knn, X_tfidf, user_id_to_iid, item_iid_to_id, train_mat, test_mat, item_n)\n",
    "\n",
    "            all_f_cb_r.append(f_cb_r)\n",
    "            all_m_cb_r.append(m_cb_r)\n",
    "\n",
    "    return (np.array(all_f_cb_r), np.array(all_m_cb_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_experiment(1, 1, listening_df, user_df, tag_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_result = cb_experiment(10, 1, listening_df, user_df, tag_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'female recall: {np.average(cb_result[0])}, std: {np.std(cb_result[0])}; male recall: {np.average(cb_result[1])}, std: {np.std(cb_result[1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_result = (np.array([0.        , 0.00714286, 0.01612903, 0.02051282, 0.        ,\n",
    "        0.        , 0.00537634, 0.00151515, 0.01794872, 0.01058201,\n",
    "        0.05191257, 0.04126984, 0.03703704, 0.0021978 , 0.02631579,\n",
    "        0.05974843, 0.        , 0.04591837, 0.01538462, 0.03694581,\n",
    "        0.        , 0.01149425, 0.00423729, 0.01969697, 0.        ,\n",
    "        0.        , 0.00704225, 0.00431034, 0.02631579, 0.00166667,\n",
    "        0.00628931, 0.01886792, 0.02380952, 0.01262626, 0.        ,\n",
    "        0.02738873, 0.02380952, 0.01010101, 0.        , 0.01960784,\n",
    "        0.0234375 , 0.04022989, 0.02037037, 0.00755858, 0.        ,\n",
    "        0.        , 0.06060606, 0.03030303, 0.        , 0.        ,\n",
    "        0.06222222, 0.00431034, 0.02419355, 0.01851852, 0.04661017,\n",
    "        0.05085784, 0.01478495, 0.02777778, 0.01639344, 0.00438596,\n",
    "        0.01785714, 0.00757576, 0.        , 0.        , 0.025     ,\n",
    "        0.00537634, 0.        , 0.03773585, 0.        , 0.00505051,\n",
    "        0.00819672, 0.01801802, 0.05555556, 0.03439153, 0.0030303 ,\n",
    "        0.00892857, 0.        , 0.01730769, 0.00606061, 0.02051282,\n",
    "        0.01397849, 0.        , 0.00701754, 0.01923077, 0.00909091,\n",
    "        0.00403226, 0.        , 0.00076923, 0.00327869, 0.03225806,\n",
    "        0.02150538, 0.00833333, 0.02051282, 0.00892857, 0.02698413,\n",
    "        0.00546448, 0.00512821, 0.01333333, 0.        , 0.03044872]),\n",
    " np.array([0.03713607, 0.0172956 , 0.        , 0.00564972, 0.00574713,\n",
    "        0.02272727, 0.00574713, 0.01694915, 0.00093284, 0.        ,\n",
    "        0.00925926, 0.        , 0.01449275, 0.01641414, 0.0122549 ,\n",
    "        0.02651515, 0.04404762, 0.00757576, 0.        , 0.0078125 ,\n",
    "        0.01944444, 0.02121212, 0.00505051, 0.02298851, 0.03825137,\n",
    "        0.04098361, 0.02525253, 0.00833333, 0.02277778, 0.02238806,\n",
    "        0.00128205, 0.02690058, 0.00396825, 0.03278689, 0.00520833,\n",
    "        0.02254098, 0.04275362, 0.        , 0.00454545, 0.01461988,\n",
    "        0.02584453, 0.00897436, 0.00373134, 0.00793651, 0.01375661,\n",
    "        0.03914141, 0.01147541, 0.00818182, 0.01272727, 0.01639344,\n",
    "        0.00862069, 0.02155172, 0.01449275, 0.01984127, 0.00861079,\n",
    "        0.02083333, 0.02595628, 0.05208333, 0.00223214, 0.01679713,\n",
    "        0.02910798, 0.        , 0.00757576, 0.01875   , 0.00995025,\n",
    "        0.        , 0.01641414, 0.02213542, 0.        , 0.        ,\n",
    "        0.03825137, 0.00511727, 0.01867816, 0.02176527, 0.03535354,\n",
    "        0.01102941, 0.00948345, 0.00292398, 0.01724138, 0.00662393,\n",
    "        0.        , 0.        , 0.03098291, 0.01442308, 0.00909091,\n",
    "        0.00874317, 0.00568182, 0.01589744, 0.01474747, 0.01904762,\n",
    "        0.02083333, 0.03173454, 0.03208812, 0.01449275, 0.        ,\n",
    "        0.00535714, 0.03333333, 0.02469136, 0.        , 0.01298425]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(cb_result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(cb_result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(cb_result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(cb_result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
