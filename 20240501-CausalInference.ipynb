{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIASMF:\n",
    "    def __init__(self, args, data_config):\n",
    "        self.n_users = data_config['n_users']\n",
    "        self.n_items = data_config['n_items']\n",
    "\n",
    "        self.decay = args.regs\n",
    "        self.emb_dim = args.embed_size\n",
    "        self.lr = args.lr\n",
    "        self.batch_size = args.batch_size\n",
    "        self.verbose = args.verbose\n",
    "        self.c = args.c\n",
    "        self.alpha = args.alpha\n",
    "\n",
    "        # placeholders\n",
    "        self.users = tf.placeholder(tf.int32, shape = (None,))     # convert to tf2\n",
    "        self.pos_items = tf.placeholder(tf.int32, shape = (None,)) # convert to tf2\n",
    "        self.neg_items = tf.placeholder(tf.int32, shape = (None,)) # convert to tf2\n",
    "\n",
    "        # initialize weights\n",
    "        self.weights = self.init_weights()\n",
    "\n",
    "        # neting\n",
    "        user_embedding = tf.nn.embedding_lookup(self.weights['user_embedding'], self.users)\n",
    "        pos_item_embedding = tf.nn.embedding_lookup(self.weights['item_embedding'], self.pos_items)\n",
    "        neg_item_embedding = tf.nn.embedding_lookup(self.weights['item_embedding'], self.neg_items)\n",
    "        user_rand_embedding = tf.nn.embedding_lookup(self.weights['user_rand_embedding'], self.users)\n",
    "        item_rand_embedding = tf.nn.embedding_lookup(self.weights['item_rand_embedding'], self.pos_items)\n",
    "        self.const_embedding = self.weights['c']\n",
    "        self.pos_item_bias = tf.nn.embedding_lookup(self.weights['item_bias'], self.pos_items)\n",
    "        self.neg_item_bias = tf.nn.embedding_lookup(self.weights['item_bias'], self.neg_items)\n",
    "\n",
    "        # prediction\n",
    "        self.batch_ratings = tf.matmul(user_embedding, pos_item_embedding, transpose_a=False, transpose_b=True)\n",
    "        self.user_const_ratings = self.batch_ratings - tf.matmul(self.const_embedding, pos_item_embedding, transpose_a=False, transpose_b=True)\n",
    "        self.item_const_ratings = self.batch_ratings - tf.matmul(user_embedding, self.const_embedding, transpose_a=False,transpose_b=True)\n",
    "        self.user_rand_ratings = self.batch_ratings - tf.matmul(user_rand_embedding, pos_item_embedding, transpose_a=False, transpose_b=True)\n",
    "        self.item_rand_ratings = self.batch_ratings - tf.matmul(user_embedding, item_rand_embedding, transpose_a=False, transpose_b=True)\n",
    "\n",
    "        self.mf_loss, self.reg_loss = self.create_bpr_loss(user_embedding, pos_item_embedding, neg_item_embedding)\n",
    "        self.loss = self.mf_loss + self.reg_loss\n",
    "\n",
    "        trainable_v1 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'parameter')\n",
    "        self.opt = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(self.loss, var_list = trainable_v1)\n",
    "\n",
    "        # two branch\n",
    "        self.w = tf.Variable(self.initializer([self.emb_dim,1]), name='item_branch')\n",
    "        # two branch bpr\n",
    "        self.mf_loss_two, self.reg_loss_two = self.create_bpr_loss_two_branch(user_embedding, pos_item_embedding, neg_item_embedding)\n",
    "        self.loss_two = self.mf_loss_two + self.reg_loss_two\n",
    "        self.opt_two = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(self.loss_two)\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        weights = dict()\n",
    "        self.initializer = tf.contrib.layers.xavier_initilizer() # convert to tf2\n",
    "        # self.initializer = tf.initializers.GlorotUniform()\n",
    "        initializer = self.initializer\n",
    "        with tf.variable_scope('parameter'):\n",
    "            weights['user_embedding'] = tf.Variable(initializer([self.n_users, self.emb_dim]), name='user_embedding')\n",
    "            weights['item_embedding'] = tf.Variable(initializer([self.n_items, self.emb_dim]), name='item_embedding')\n",
    "            weights['user_rand_embedding'] = tf.Variable(initializer([self.n_users, self.emb_dim]), name='user_rand_embedding', trainable=False)\n",
    "            weights['item_rand_embedding'] = tf.Variable(initializer([self.n_items, self.emb_dim]), name='item_rand_embedding', trainable=False)\n",
    "            weights['item_bias'] = tf.Variable(initializer([self.n_items]), name='item_bias')\n",
    "        with tf.variable_scope('const_embedding'):\n",
    "            self.rubi_c = tf.Variable(tf.zeros([1]), name='rubi_c')\n",
    "            weights['c'] = tf.Variable(tf.zeros([1, self.emb_dim]), name='c')\n",
    "        return weights\n",
    "    \n",
    "    def create_bpr_loss(self, users, pos_items, neg_items):\n",
    "         # users, pos_items, neg_items have the same shape,\n",
    "         # since we only sample one pos item (i) and one neg item (j) for each user\n",
    "        pos_scores = tf.reduce_sum(tf.multiply(users, pos_items), axis=1) + self.pos_item_bias # yhat_ui\n",
    "        neg_scores = tf.reduce_sum(tf.multiply(users, neg_items), axis=1) + self.neg_item_bias # yhat_uj\n",
    "\n",
    "        # BPR Loss: L_O = sum_{i,j,u} -log(sigmoid(yhat_ui - yhat_uj))\n",
    "        maxi = tf.math.log(tf.nn.sigmoid(pos_scores - neg_scores))\n",
    "        mf_loss = tf.negative(tf.reduce_mean(maxi))\n",
    "\n",
    "        # regularize\n",
    "        regularizer = tf.nn.l2_loss(users) + tf.nn.l2_loss(pos_items) + tf.nn.l2_loss(neg_items)\n",
    "        regularizer = regularizer/self.batch_size\n",
    "        reg_loss = self.decay * regularizer\n",
    "\n",
    "        return mf_loss, reg_loss\n",
    "    \n",
    "    def create_bpr_loss2(self, users, const_embedding, pos_items, neg_items):\n",
    "        pos_scores = tf.reduce_sum(tf.multiply(users, pos_items), axis=1) - tf.matmul(const_embedding, pos_items, transpose_a=False, transpose_b=False)\n",
    "        neg_scores = tf.reduce_sum(tf.multiply(users, neg_items), axis=1) - tf.matmul(const_embedding, neg_items, transpose_a=False, transpose_b=False)\n",
    "\n",
    "        maxi = tf.log(tf.nn.sigmoid(pos_scores - neg_scores))\n",
    "        mf_loss = tf.negative(tf.reduce_mean(maxi))\n",
    "\n",
    "        regularizer = tf.nn.l2_loss(const_embedding)\n",
    "        regularizer = regularizer/self.batch_size\n",
    "        reg_loss = self.decay * regularizer\n",
    "        \n",
    "        return mf_loss, reg_loss\n",
    "        \n",
    "    def create_bce_loss(self, users, pos_items, neg_items):\n",
    "        pos_scores = tf.reduce_sum(tf.multiply(users, pos_items), axis=1) + self.pos_item_bias # yhat_ui\n",
    "        neg_scores = tf.reduce_sum(tf.multiply(users, neg_items), axis=1) + self.neg_item_bias # yhat_ui\n",
    "\n",
    "        # BCE Loss: L_O = sum_{u,i} (-y_ui * log(sigmoid(yhat_ui)) - (1-y_ui)* log(1 - sigmoid(yhat_ui)))\n",
    "        mf_loss = tf.reduce_mean(tf.negative(tf.math.log(tf.nn.sigmoid(pos_scores)+1e-9)) + tf.negative(tf.math.log(1-tf.nn.sigmoid(neg_scores)+1e-9)))\n",
    "\n",
    "        regularizer = tf.nn.l2_loss(users) + tf.nn.l2_loss(pos_items) + tf.nn.l2_loss(neg_items)\n",
    "        regularizer = regularizer/self.batch_size\n",
    "        reg_loss = self.decay * regularizer\n",
    "\n",
    "        return mf_loss, reg_loss\n",
    "    \n",
    "    def create_bce_loss2(self, users, const_embedding, pos_items, neg_items):\n",
    "        pos_scores = tf.reduce_sum(tf.multiply(users, pos_items), axis=1) - tf.matmul(const_embedding, pos_items, transpose_a=False, transpose_b=False)\n",
    "        neg_scores = tf.reduce_sum(tf.multiply(users, neg_items), axis=1) - tf.matmul(const_embedding, neg_items, transpose_a=False, transpose_b=False)\n",
    "\n",
    "        mf_loss = tf.reduce_mean(tf.negative(tf.math.log(tf.nn.sigmoid(pos_scores)+1e-9)) + tf.negative(tf.math.log(1 - tf.nn.sigmoid(neg_scores)+1e-9)))\n",
    "\n",
    "        regularizer = tf.nn.l2_loss(const_embedding)\n",
    "        regularizer = regularizer/self.batch_size\n",
    "        reg_loss = self.decay * regularizer\n",
    "\n",
    "        return mf_loss, reg_loss\n",
    "        \n",
    "    \n",
    "    def create_bpr_loss_two_branch(self, users, pos_items, neg_items):\n",
    "        pos_scores = tf.reduce_sum(tf.multiply(users, pos_items), axis=1) + self.pos_item_bias # yhat_k\n",
    "        neg_scores = tf.reduce_sum(tf.multiply(users, neg_items), axis=1) + self.neg_item_bias # yhat_k\n",
    "\n",
    "        pos_items_stop = pos_items\n",
    "        neg_items_stop = neg_items\n",
    "        self.pos_item_scores = tf.matmul(pos_items_stop, self.w) # yhat_i\n",
    "        self.neg_item_scores = tf.matmul(neg_items_stop, self.w) # yhat_j\n",
    "\n",
    "        #first branch: L_O\n",
    "        pos_scores = pos_scores*tf.nn.sigmoid(self.pos_item_scores) # yhat_ui = yhat_k * sigmoid(yhat_i)\n",
    "        neg_scores = neg_scores*tf.nn.sigmoid(self.neg_item_scores) # yhat_uj = yhat_k * sigmoid(yhat_j)\n",
    "\n",
    "        self.rubi_ratings = (self.batch_ratings - self.rubi_c) * tf.squeeze(tf.nn.sigmoid(self.pos_item_scores))\n",
    "        self.direct_minus_ratings = self.batch_ratings - self.rubi_c * tf.squeeze(tf.nn.sigmoid(self.pos_item_scores))\n",
    "        # BPR Loss: L_O = sum_{i,j,u} -log(sigmoid(yhat_ui - yhat_uj))\n",
    "        maxi = tf.math.log(tf.nn.sigmoid(pos_scores - neg_scores))\n",
    "        self.mf_loss_ori_bce = tf.negative(tf.reduce_mean(maxi))\n",
    "        \n",
    "        # second branch: L_I\n",
    "        # BPR Loss: L_I = sum -log(sigmoid(yhat_i - yhat_j))\n",
    "        maxi_item = tf.math.log(tf.nn.sigmoid(self.pos_item_scores - self.neg_item_scores))\n",
    "        self.mf_loss_item_bce = tf.negative(tf.reduce_mean(maxi_item))\n",
    "\n",
    "        # unify\n",
    "        mf_loss = self.mf_loss_ori_bce + self.alpha * self.mf_loss_item_bce\n",
    "\n",
    "        # regularize\n",
    "        regularizer = tf.nn.l2_loss(users) + tf.nn.l2_loss(pos_items) + tf.nn.l2_loss(neg_items)\n",
    "        regularizer = regularizer/self.batch_size\n",
    "\n",
    "        reg_loss = self.decay + regularizer\n",
    "\n",
    "        return mf_loss, reg_loss\n",
    "\n",
    "    def create_bce_loss_two_branch(self, users, pos_items, neg_items):\n",
    "        pos_scores = tf.reduce_sum(tf.multiply(users, pos_items), axis=1) + self.pos_item_bias # yhat_k\n",
    "        neg_scores = tf.reduce_sum(tf.multiply(users, neg_items), axis=1) + self.neg_item_bias # yhat_k\n",
    "\n",
    "        pos_items_stop = pos_items\n",
    "        neg_items_stop = neg_items\n",
    "        self.pos_item_scores = tf.matmul(pos_items_stop, self.w) # yhat_i\n",
    "        self.neg_item_scores = tf.matmul(neg_items_stop, self.w) # yhat_i\n",
    "\n",
    "        # first branch: L_O\n",
    "        # fusion\n",
    "        pos_scores = pos_scores * tf.nn.sigmoid(self.pos_item_scores) # yhat_ui = yhat_k * sigmoid(yhat_i)\n",
    "        neg_scores = neg_scores * tf.nn.sigmoid(self.neg_item_scores) # yhat_ui = yhat_k * sigmoid(yhat_i)\n",
    "        # BCE Loss: L_O = sum_{u,i} (-y_ui * log(sigmoid(yhat_ui)) - (1-y_ui)* log(1 - sigmoid(yhat_ui)))\n",
    "        self.mf_loss_ori = tf.reduce_mean(tf.negative(tf.math.log(tf.nn.sigmoid(pos_scores)+1e-10)) + tf.negative(tf.math.log(1-tf.nn.sigmoid(neg_scores)+1e-10)))\n",
    "\n",
    "        # second branch: L_I\n",
    "        # BCE Loss: L_I = sum_{u,i} (-y_ui * log(sigmoid(yhat_i)) - (1-y_ui)* log(1 - sigmoid(yhat_i)))\n",
    "        self.mf_loss_item = tf.reduce_mean(tf.negative(tf.math.log(tf.nn.sigmoid(self.pos_item_scores)+1e-10)) + tf.negative(tf.math.log(1-tf.nn.sigmoid(self.neg_item_scores)+1e-10)))\n",
    "\n",
    "        # unify\n",
    "        # L = L_O + alpha * L_I\n",
    "        mf_loss = self.mf_loss_ori + self.alpha * self.mf_loss_item\n",
    "\n",
    "        # regularize\n",
    "        regularizer = tf.nn.l2_loss(users) + tf.nn.l2_loss(pos_items) + tf.nn.l2_loss(neg_items)\n",
    "        regularizer = regularizer/self.batch_size\n",
    "        reg_loss = self.decay * regularizer\n",
    "\n",
    "        return mf_loss, reg_loss\n",
    "    \n",
    "    def update_c(self, sess, c):\n",
    "        sess.run(tf.assign(self.rubi_c, c*tf.ones([1])))\n",
    "\n",
    "    def _statistics_params(self):\n",
    "        # number of params\n",
    "        total_parameters = 0\n",
    "        for variable in self.weights.values():\n",
    "            shape = variable.get_shape()\n",
    "            variable_parameters = 1\n",
    "            for dim in shape:\n",
    "                variable_parameters *= dim.value\n",
    "            total_parameters += variable_parameters\n",
    "        if self.verbose > 0:\n",
    "            print(\"#params: %d\" % total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = tf.constant([[1.0,1.0,1.0],[2.0,2.0,2.0],[3.0,3.0,3.0]])\n",
    "pos_items = tf.constant([[1.0,1.0,1.0],[10.0,10.0,10.0],[100.0,100.0,100.0]])\n",
    "neg_items = tf.constant([[-1.0,1.0,-1.0],[-10.0,10.0,-10.0],[-100.0,100.0,-100.0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_scores = tf.reduce_sum(tf.multiply(users, pos_items), axis=1) \n",
    "neg_scores = tf.reduce_sum(tf.multiply(users, neg_items), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([  3,  60, 900], dtype=int32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(tf.multiply(users, pos_items), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
